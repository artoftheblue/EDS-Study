\documentclass[a4paper, 10pt]{article}
\usepackage{header}

\title{\LARGE{Теория вероятностей и математическая статистика—1}}
\author{Винер Даниил  \href{https://t.me/danya_vin}{@danya\_vin}}
\date{Версия от \today}
\begin{document}
\maketitle
\tableofcontents
\setlength{\parindent}{15pt}
\setlength{\parskip}{2mm}
\setlist[itemize]{left=1cm}
\setlist[enumerate]{left=1cm}
\newpage
\section{Дискретное вероятностное пространство. Базовые теоремы вероятности}
\definition $\Omega=\{\omega_1,\ldots,\omega_k,\ldots\}$ называется \textit{пространством элементарных исходов}, где $w_i$ — элементарный исход\\[2mm]
\definition $A$ — любое подмножество $\Omega$\\[2mm]
\definition Событие называется \textit{достоверным}, если $A=\Omega$\\[2mm]
\comment К $A$ применимы те же опреации, что используются с множествами\\[2mm]
\definition Полная группа несовместных событий — такой набор событий, для которого выполняются такие условия:
\begin{equation*}
    A_i\cap A_j=\varnothing\ \forall i\ne j
\end{equation*}
\begin{equation*}
    \displaystyle\bigcup\limits_i A_i=\Omega
\end{equation*}

\axiom $\forall \omega_i\ \exists p_i\geqslant0,$ при этом $\displaystyle\sum_i p_i=1$\\[2mm]
\corollary $0\leqslant p_i\leqslant1$\\[2mm]
\definition $P(A)=\sum_{w_i\in A} P(w_i)$, где $P(w_i)=p_i$\\[2mm]
\indent $(\Omega, P)$ — вероятностное пространство в дискретном случае\\[2mm]
\textbf{Подходы к определению вероятностей}
\begin{enumerate}
    \item Априорный (предварительное знание)
    \item Частотный (предел ряда частот)
    \item Модельный (математическая модель)
\end{enumerate}
\subsection{Классическое определение вероятности}
\indent Имеет место, когда исходы равновероятны\\[2mm]
\definition
$$P(A)=\displaystyle\frac{|A|}{|\Omega|}$$

\definition $P(A)=\sum_{\omega_i\in A}p(\omega_i)$
\subsection{Теорема сложения}
\indent $P(A\cup B)=P(A)+P(B)-P(A\cap B)$\\[2mm]
\indent $P(A_1\cup\ldots\cup A_n)=\sum_{i=1}^{n} P(A_i)-\sum_{i<j} (A_i\cap A_j)+\ldots+(-1)^{n-1}P(A_1\cap\ldots\cap A_n)$
\subsection{Условная вероятность}
$P(A|B)=\displaystyle\frac{P(A\cap B)}{P(B)}\ \forall B: P(B)>0$
\subsection{Теорема умножения}
$P(A_1\cap\ldots\cap A_n)=P(A_1)\cdot P(A_2|A_1)\cdot P(A_3|A_1\cap A_2)\cdot\ldots\cdot P(A_n|A_1\cdot\ldots\cdot A_{n-1})$

\section{Независимые события. Апостериорная и условная вероятности}
Напомним классическое определение вероятности
$$P(A)=\sum_{\omega_i\in A}p(\omega_i)$$

\comment $P(\overline{A})=1-P(A),\ P(\Omega)=1$

\subsection{Задача об авариях}
Известно, что в $40\%$ аварий виноваты пьяные водители. Утверждается, что из этого следует, что в $60\%$ случаев виноваты трезвые водители

Пусть есть такие события: $A$ — водитель пьян, $B$ — водитель трезв, $C$ — авария случилась

Формально, задача выглядит так: $P(A|C)=0.4\Longrightarrow P(B|C)=0.6$

Пусть $P(B)=0.05$

$\displaystyle\frac{P(C|A)}{P(C|B)}=\frac{P(C\cap A)/P(A)}{P(C\cap B)}=\frac{P(A|C)P(C)}{P(B|C)P(C)}\cdot\frac{P(B)}{P(A)}=\frac{0.4}{0.6}\cdot\frac{0.95}{0.05}\approx 12.7$

\subsection{Независимость событий}
\definition (интуитивное) События  $A\text{ и }B$ \textbf{независимы}, если $P(A|B)=P(A)$

\definition События $A\text{ и }B$ называются попарно независимыми, если:
\begin{equation*}
    \begin{aligned}
        P(A\cap B)&=P(A)\cdot P(B)\\
        P(A|B)P(B)&=P(A)\cdot P(B)\text{ — вытекает интуитивное определение}
    \end{aligned}
\end{equation*}

\definition События $A_1,\ldots, A_n$ независимы в совокупности, если:
\begin{equation*}
    \begin{aligned}
    \forall i_1<\ldots<i_k<\ldots<i_n\ \forall k=1,\ldots,n:\\ P(A_{i_1}\cap A_{i_2}\cap\ldots\cap A_{i_k})=P(A_{i_1})\cdot P(A_{i_2})\cdot \ldots\cdot P(A_{i_k})
    \end{aligned}
\end{equation*}

\comment  Для $A_1,A_2,A_3$:
\begin{equation*}
    \begin{aligned}
        P(A_1\cap A_2)&=P(A_1)\cdot P(A_2)\\
        P(A_2\cap A_3)&=P(A_2)\cdot P(A_3)\\
        P(A_1\cap A_3)&=P(A_1)\cdot P(A_3)\\
        P(A_1\cap A_2\cap A_3)&=P(A_1)\cdot P(A_2)\cdot P(A_3)
    \end{aligned}
\end{equation*}

\subsection{Пример зависимых событий в совокупности}
Положим, что у нас есть тетраэдр, каждая сторона которого покрашена в некоторые комбинации цветов: $A$ — красный, $B$ — желтый, $C$ — зеленый, а четвертая покрашена во все три цвета, тогда вероятности:
\begin{equation*}
    \frac{1}{4}=P(A\cap B\cap C)\ne P(A)\cdot P(B)\cdot P(C)=\frac{1}{8}
\end{equation*}
Таким образом, события зависимы в совокупности

\comment Если $A_1,\ldots, A_n$ независимы в совокупности, то над любым из событий можно поставить знак отрицания, тогда система останется независимой

\ex Есть события $A_1,A_2,A_3$ — независимы в совокупности, тогда $\overline{A_1},A_2,A_3$ — тоже независимы в совокупности

Проверим данный пример. 

\begin{equation*}
    \begin{aligned}
        P(\overline{A_1}\cap A_2\cap A_3)&=P((A_2\cap A_1)\setminus A_1)\\
        &=P(A_2)P(A_3)-P(A_1\cap A_2\cap A_3)\\
        &=P(A_2)P(A_3)-P(A_1)P(A_2)P(A_3)\\
        &=P(\overline{A_1})P(A_2)P(A_3)
    \end{aligned}
\end{equation*}
Теперь разберемся с двумя событиями
\begin{equation*}
    \begin{aligned}
        P(\overline{A_1}\cap A_2)&=P(\overline{A_1})P(A_2)\\
        P(\overline{A_2}\cap A_1)&=P(A_2\setminus A_1)\\
        &=P(A_2)-P(A_2\cap A_1)=P(A_2)-P(A_2)P(A_1)\\
        &=P(A_2)(1-P(A_1))
    \end{aligned}
\end{equation*}

\subsection{Простейший варинат ЗБЧ. Неизбежность технологических катастроф}
Имеется $n$ узлов, а $A_1,\ldots,A_n$ — события, где $A_i$ означает, что $i$-ый узел вышел из строя

Очевидно, что $P(\text{хотя бы один узел выйдет из строя})=P(A_1\cup \ldots \cup A_n)$, тогда $\forall i: 0<\varepsilon\leqslant P(A_i)<1$ выполняется:

% Вероятность стремится к 1 при увеличении количества узлов
$$1\geqslant P(A_1\cup \ldots \cup A_n)=1-P(\overline{A_1}\cap \ldots\cap\overline{A_n})=1-\displaystyle\prod_{i=1}^{n} \underbrace{P(\overline{A_i})}_{\leqslant 1-\varepsilon}\geqslant \lim\limits_{n\mapsto\infty}1-(1-\varepsilon)^n=1$$
% где каждое $P(\overline{A_i})=1-P(A_i)\leqslant 1-\varepsilon$

Это означает, что вероятность технологических катастроф при количестве узлов, стремящемся в бесконечность, равна $100\%$
\subsection{Формула полной вероятности}
Пусть $\{H_i\}$ — полная группа несовместных событий (разбиение $\Omega$)

Некоторые свойства:
\begin{itemize}
    \item $H_i\cap H_j=\varnothing\ \forall i\ne j$ — несовместность\\[1mm]
    \item $\displaystyle\bigcup_{i=1}^{n}H_i=\Omega$ — полнота
\end{itemize}

\theorem Тогда, $P(A)=\sum_{i=1}^{n}P(A|H_i)P(H_i)$

\proof
\begin{equation*}
    \begin{aligned}
        P(A)&=P\left(\bigcup_{i=1}^{n}(A\cap H_i)\right)\\
        &=\sum_{i=1}^{n}P(A\cap H_i)\\
        &=\sum_{i=1}^{n} P(A|H_i)\cdot P(H_i)
    \end{aligned}
\end{equation*}\qed

\subsection{Формула Байеса. Апосториорные вероятности}
\begin{equation*}
    \begin{aligned}
        P(H_k|A)&=\frac{P(A|H_k)\cdot P(H_k)}{P(A)}\\
        &=\frac{P(A|H_k)\cdot P(H_k)}{\sum_{i=1}^{n} P(A|H_i)P(H_i)}
    \end{aligned}
\end{equation*}

\subsection{Задача про неизличимые заболевания}
Требуется вычислить, какова вероятность, что человек, который получил положительный тест на СПИД, на самом деле не болен

$P(\text{СПИД})=0.03$ — вероятность быть носителем СПИДа

$P(+|\text{ СПИД})=0.98$ — чувствительность теста, т.е. тест будет положительным, если у человека есть СПИД, с вероятностью $0.98$

$P(+|\overline{\text{СПИД}})=0.01=1-\text{специфичность}$, т.е. тест будет положительным, если у человека нет СПИДа, с вероятностью $0.01$

\begin{equation*}
    \begin{aligned}
        P(\text{СПИД}|+)&=\displaystyle\frac{P(+|\text{СПИД})P(\text{СПИД})}{P(+|\text{СПИД})P(\text{СПИД})+P(+|\overline{\text{СПИД}})P(\overline{\text{СПИД}})}\\
        &=\frac{0.98\cdot0.03}{0.98\cdot0.03+0.01\cdot0.97}\\
        &\approx0.75
    \end{aligned}
\end{equation*}
То есть, \textbf{каждый четвертый} человек с положительным СПИД-тестом \textbf{здоров}

\section{Схема Бернулли. Теорема Пуассона. Случайные величины}
Рассмотрим задачу проведения социологических опросов. Допустим, проводится опрос <<Употребляет ли человек наркотики?>>. Предполагается, что не все респонденты, даже на условиях анонимности, дадут честный ответ. Для решения этой проблемы иногда задаются косвенные вопросы, мы же рассмотрим следующий подход.

Пусть человек перед ответом берет из урны случайный шарик. В урне лежит $N$ шариков трех цветов: белый, черный и красный. Если человек достал белый, то его ответ на вопрос просто число <<0>>, если черный — <<1>>, если красный — респондент должен сказать правду: употребляет ли он наркотики. Тогда, все ответы разделятся на три категории, которые можно описать, как
$$N=N_\text{к}+N_{\text{ч}}+N_{\text{б}}$$

Пусть $p$ — вероятность употребления наркотиков респондентом, тогда по формуле полной вероятности получаем
$$P(1)=P(1|\text{б})P(\text{б})+P(1|\text{ч})P(\text{ч})+P(1|\text{к})P(\text{к})$$

Отсюда следует, что $p=\displaystyle\frac{P(1)N-N_{\text{ч}}}{N_{\text{к}}}$

\ex Если оказалось, что $P(1)=0.6$, это означает, что $p=\frac{1}{2}$

\subsection{Схема Бернулли. Биномиальное распредление вероятности}
Допустим, что мы проводим $n$ независимых опытов, каждый из которых может закончится успехом (1) или неудачей (0). Пусть вероятность успеха $P(1)=p$, тогда $P(0)=1-p=q$

Упорядочим результаты экспериментов в последовательность длины $n$, тогда всего исходов $|\Omega|=2^n$, то есть последоватльности длины $n$, состоящие из 0 и 1. Заметим, что они \textbf{НЕ равновероятны}

Пользуясь тем, что события независимы, получаем, что $$P(\underbrace{1,\ldots,1}_{k},\underbrace{0,\ldots,0}_{n-k})=p^k(1-p)^{n-k}$$

Заметим, что исходов, в которых $k$ успехов, существует $C_n^k$, при этом
$$P(\text{$k$ успехов в $n$ испытаниях})=C_n^k p^k(1-p)^{n-k}$$

\definition Набор чисел $\{C_n^k p^k(1-p)^{n-k}|k=0,1,\ldots,n\}$ называется \textbf{биномиальным распределением вероятности}

\subsection{Исследование Пуассона}
Французский математик Симеон Дэни Пуассон проводил исследование, в котором пытался понять насколько справедливы обвинительные приговоры, которые выносят судьи присяжных во Франции.

Присяжных $n=12$ человек. Для того, чтобы человек был признан виновынм, нужно 7 голосов <<ЗА>>. Пуассон знал лишь процент обвинительных приговоров и количество обвинительных приговоров, вынесенных ровно 7 присяжными заседателями. Пусть $p$ — вероятность верного решения присяжного (неизвестный нам параметр). Верное решение — когда улик достаточно для обвинения - присяжный голосует за \textit{виновность}, не достаточно — за \textit{невиновность}

Пусть есть событие $A$ — улик достаточно для обвинения. Заметим, что $P(A)$ нам неизвестно.

Очевидно, что частота обвинительных приговоров$=P(\text{обвинение})$, тогда

$$P(\text{обвинение})=P(\text{обвинение}|A)P(A)+P(\text{обвинение}|\overline{A})P(\overline{A})$$

Теперь введем вероятность, что человек был обвинен, когда за это проголосовало ровно 7 присяжных:
\begin{equation*}
    \begin{aligned}
        P(\text{обв. 7})&=P(\text{обв. 7}|A)P(A)+P(\text{обв. 7}|\overline{A})P(\overline{A})\\
        &=C_{12}^7 p^7(1-p)^5P(A)+C_{12}^5(1-p)^7p^5(1-P(A))
    \end{aligned}
\end{equation*}

Тогда, $P(\text{обвинение})=\left(\sum_{k=7}^{12} C_{12}^k p^k(1-p)^{12-k}\right)P(A)+\left(\sum_{k=7}^{12} C_7^{12-k}(1-p)^kp^{12-k}\right)(1-P(A))$

Пуассон решил, что $p\approx\displaystyle\frac{2}{3}$. Однако, это было ошибочным решением. Математик считал, что присяжные выносят решения независимо друг от друга, что в корни неверно. А так как Пуассон опирался на схему Бернулли, которая работает только для \textit{независимых событий}, то и результат его исследования получился ложным

При этом, Пуассона не смутило, что в Англии для вынесения обвинительного приговора нужно, чтобы все 12 присяжных проголосавали за обвинение. Если бы решение Пуассона было верным, тогда вероятность обвинительных приговоров должна быть $\approx\left(\frac{2}{3}\right)^{12}$, что не соответствовало действительности, ведь в Англии было больше обвинительных приговоров чем во Франции

\subsection{Теорема Пуассона}
\theorem Пусть имеются серии испытаний. В каждой серии $n$ испытаний и $p_n$ — вероятность успеха. Также, пусть $n\rightarrow\infty$ и $p_n\rightarrow0$, причем $np_n\rightarrow\lambda$

Тогда, 
\begin{equation*}
    \forall k\ P(k\text{ успехов в $n$ испытаниях})=C_n^kp^k(1-p)^{n-k}\rightarrow\frac{\lambda^k}{k!}e^{-\lambda}
\end{equation*}

\proof Пусть $\mu_n$ — число успехов в $n$ испытаниях Бернулли с вероятностью успеха $p_n$, а также $\lambda_n=n\cdot p_n$. По условию, $\lambda^k_n\rightarrow\lambda>0$. Подставим $p_n=\displaystyle\frac{\lambda_n}{n}$ в формулу Бернулли. Тогда,

\begin{equation*}
    \begin{aligned}
        P(\mu_n=m)&=\displaystyle\frac{n!}{m!(n-m)!}p_n^m (1-p_n)^{n-m}\\
        P(\mu_n=m)&=\displaystyle\frac{n\cdot\ldots\cdot(n-m+1)}{n^k}\left(\frac{\lambda_n}{m}\right)^m\frac{(1-\frac{\lambda_n}{n})^n}{(1-\frac{\lambda_n}{n})^m}\\
        &=\displaystyle\frac{n\cdot\ldots\cdot(n-m+1)}{n^k}\left(\frac{\lambda_n}{m}\right)^m\left(1-\frac{\lambda_n}{n}\right)^n\cdot\frac{1}{(1-\frac{\lambda_n}{n})^m}\\
        &=\underbrace{\displaystyle\frac{n\cdot\ldots\cdot(n-m+1)}{n^k}}_{\lim\limits_{n\rightarrow\infty}=1}\frac{\lambda_n^m}{m!}\underbrace{\left(1-\frac{\lambda_n}{n}\right)^n}_{e^{-\lambda}}\cdot\underbrace{\left(1-\frac{\lambda_n}{n}\right)^{-m}}_{\lim\limits_{n\rightarrow\infty}=1}\\
        &=\frac{\lambda^k}{k!}e^{-\lambda}
    \end{aligned}
\end{equation*}

\definition Набор чисел $\left\{\frac{\lambda^k}{k!}e^{-\lambda}\colon k=0,1,\ldots,n\right\}$ называется \textbf{распределением Пуассона}

\comment Величина $\frac{\lambda^k}{k!}e^{-\lambda}$ устойчива при зависимых событиях, а также при ошибках предпосылок (таких, какие допустил Пуассон)

\comment $\forall B$ — множество значений, которые может принимать число успехов
$$\left|P(\mu\in B)-\sum_{m\in B}e^{-\lambda}\frac{\lambda^m}{m!}\right|\leqslant\min{(p,np^2)}$$

\subsection{Случайные величины. Математическое ожидание}
\definition Случайная величина $\xi:\Omega\rightarrow\mathbb{R}$

$\xi$ имеет биномиальное распределение, если
$$\xi\in\{0,1,\ldots,n\}\ P(\xi=k)=C_n^k p^k(1-p)^{n-k},$$
где $\{0,1,\ldots,n\}$ — число успехов в $n$ испытаниях

$P(\xi\in B)=\sum_{\omega} P(\omega:\xi(\omega)\in B)$

\definition Математическим ожиданием величины $\xi$ называется величина $$\mathbb{E}(\xi)=\sum_{\omega} P(\xi)\cdot\xi(\omega),$$
если этот ряд сходится абсолютно, то есть $\sum\left|\xi(\omega)\right|P(\omega)$

\theorem $\mathbb{E}(\xi)=\sum_{k}a_k P(\xi=a_k)$ 

% (доказать)

% Доказать, что $\mathbb{E}(\xi+\nu)=\mathbb{E}\xi+\mathbb{E}\nu$


% Эта штука устойчива при зависимых событиях, а также при ошибках предпосылок

% Эта вероятность называется \textbf{распределением Пуассона}

\section{Основные дискретные распределения. Математическое ожидание}
Напомним определение случайной величины — $\xi:\Omega\rightarrow\mathbb{R}$

\definition Дискретное вероятностное пространство — $(\Omega,P)\rightarrow(\mathbb{R},P(\xi))$, где $P\xi:P(\xi=a)\ \forall a\in\mathbb{R}$

Если $\xi\in a_1\ldots a_k$, а вероятности равны $p_1\ldots p_k$, где $p_i=P(\xi=a)$, тогда
$$P(\xi=a)=P\left(\underbrace{\{\omega:\xi(\omega)=a_i\}}_{A\text{-прообраз }\{\xi=a_i\}}\right)\text{ — распределение вероятностей случайной величины }\xi$$

\ex $\xi:0,1,\ldots,n$, $p:P(\xi=k)=C_n^k p^k(1-p)^{n-k}$ — биномиальное распредеденме

Обозначение: $Bi(n,p)$

\definition $\mathbb{E}(\xi)=\sum_{\omega\in\Omega} \xi(\omega)P(\omega)$ — если ряд сходится абсолютно

Если ряд не сходится, то математического ожидания \textbf{не существует}

\subsection{Основные дискретные распределения}
\begin{enumerate}
    \item Вырожденное распределение. $\exists c:P(\xi=c)=1,\ c\in\mathbb{R}$
    \item Распределение Бернулли: $Bi(1,p)$. $\xi\in[0,1]$ с вероятностями $p$ и $1-p$ соответственно

    Индикатор события $A$ — $I\{A\}=\begin{cases}
        1,\text{ если }A\\
        0,\text{ если }\overline{A}
    \end{cases}$

    \ex 10 кубиков, $A$ — хотя бы одна 6

    $I\{A\}=\begin{cases}
        1, P=1-\left(\frac{5}{6}\right)^{10}\\
        0, P=\left(\frac{5}{6}\right)^{10}
    \end{cases}$
    \item Биномиальное распределение $Bi(n,p)$, где $\mu_n$ — число успехов в $n$ испытаниях

    $P(\mu_n=k)=C_n^kp^{k}(1-p)^{n-k},k=\overline{0,n}$

    $\mathbb{E}(\mu_n)=\sum_{k=0}^{n}kC_n^kp^k(1-p)^{n-k}$

    \item Распределение Пуассона: $\Pi(\lambda)$

    $P(\xi=k)=\displaystyle\frac{\lambda^k}{k!}e^{-\lambda},k\in\mathbb{Z}$

    Иногда это распределение называют \textit{распределением редких событий}

    $\mathbb{E}(\xi)=\lambda\ (\lambda=np)$, где $\mathbb{E}(\xi)=\sum_{k=0}^{\infty} k\displaystyle\frac{\lambda^k}{k!}e^{-\lambda}$

    \item Геометрическое распределение
    
    \ex Число неудач до первого успеха = $\xi-1$ \textbf{или} номер первого успеха = $\xi$
    
    При этом, $p$ — вероятность успеха

    $\xi:1,\ldots$. $P(\xi=k)=(1-p)^{k-1}p$

     $\mathbb{E}(\xi)=\sum_{k=1}^{\infty} k(1-p)^{k-1}p$

     Пусть $\mu=\mathbb{E}(\xi)$ — мат.ожидание номера первого успеха
     
     Тогда, $\mu=1\cdot p+(1-p)(1+\mu)\Longrightarrow\mu=\frac{1}{p}$

     \item Отрицательное биномиальное распределение: $\overline{Bi}(r,p)$. Номер $r$-го успеха, $k$-е место

    $P(\xi=k)=C_{k-1}^{k-r}\ p^{r-1}(1-p)^{k-r}\cdot p$

    \item Гипергеометрическое распределение

    $P(\xi=k)=\displaystyle\frac{C_{M}^{k}C_{N-M}^{n-k}}{C_n^k}$
\end{enumerate}

\subsection{Свойства линейности мат.ожидания}
Пусть $\mu_n=\sum_{i=1}^{n}I\{A_i\}$, где $A_i$ — в $i$-ом опыте был успех

$\mathbb{E}(I\{A_i\})=P(A_i)=p$, тогда, $\mathbb{E}(\mu_n)=np$

$np-q\leqslant\text{(наиболее вероятное число успехов)}\leqslant np+p$


\section{Аксиоматика Колмогарова}
\state Всякий непротиворечивый веротяностный эксперимпент может быть заисан на языке теории меры

\definition Пусть задана непустное множество $\Omega$. Система подмножеств $\fk$ называется \s-алгеброй, если
\begin{itemize}
    \item $\Omega\in\fk$
    \item если $A\in\fk$, то $A^c\in\fk$
    \item если $A_1,\ldots,A_n\in\fk$, то $\displaystyle\bigcup_{i=1}^{\infty} A_i\in\fk$
\end{itemize}

При этом, $\Omega$ называется единицей \s-алгебры $\fk$

\ex $\Omega=\{a,b,c,d\}$, $\fk=\{\varnothing,\Omega\}$ — \s-алгебра?
Первые два условия выполняются тривиально, а третье
\begin{equation*}
    \bigcup_{i=1}^{\infty}A_i=
    \begin{cases}
    \varnothing \in\fk\text{, если все $A_i=\varnothing$}\\
    \Omega\in\fk\text{, если хотя бы один из $A_i=\Omega$}
\end{cases}
\end{equation*}

\ex $\Omega=\{a,b,c,d\}$, $\fk=\{\varnothing,\Omega,\{a,b\}\}$ — \s-алгебра?

Нет, так как $\{a,b\}\in\fk$, но $\{a,b\}=\{c,d\}\notin\fk$

\state Пусть $\fk$ — \s-алгебра, тогда если $B,C\in\fk$, то
\begin{itemize}
    \item $B\cup C\in\fk$
    
    \proof $B\cup C\cup \varnothing \cup \varnothing\ldots\cup \varnothing\ldots\in\fk$
    \item $B\cap C\in\fk$
    
    \proof $B\cap C=\left(B^c\cup C^c\right)^c\in\fk$ по предыдущему пункту и свойству \s-алгебры №2
    \item $B\setminus C\in\fk$
    
    \proof $B\cap C^c\in\fk$
\end{itemize}

\state Пусть $\fk$ — \s-алгебра, тогда если $A_1,\ldots,A_n\ldots\in\fk$, то $\displaystyle\bigcap_{i=1}^{\infty}A_i\in\fk$

\proof $\displaystyle\bigcap_{n=1}^{\infty}A_n=\underbrace{\bigcup_{n=1}^{\infty}\underbrace{A_n^c}_{\in\fk}}_{\in\fk}\in\fk$

Пусть $\omega\in\displaystyle\bigcap A_i\Longleftrightarrow(\forall i\in\mathbb{N}\ \omega\in A_i)\Longleftrightarrow(\forall i\in\mathbb{N}\ \omega\notin A_i^c)\Longleftrightarrow(\omega\notin\displaystyle\bigcup_{i=1}^{\infty} A_i^c)\Longleftrightarrow\omega\in(\displaystyle\bigcup A_i^c)^c$

\definition Пусть $\Omega\neq\varnothing$ и $\mathcal{S}$ — это непустая система подмножеств множества $\Omega$

\textbf{Минимальной \s-алгеброй}, содержащей систему $\mathcal{S}$ называется такая \s-алгебра $\sigma(\mathcal{S})$, что
\begin{itemize}
    \item $\mathcal{S}\subseteq\sigma(\mathcal{S})$
    \item $\forall$ \s-алгебры $\mathcal{G}$, которая содержит систему $\mathcal{S}$ $(\mathcal{S}\subseteq\mathcal{G})$ справедливо, что $\sigma(\mathcal{S})\subseteq\mathcal{G}$
\end{itemize}

\definition Минимальная \s-алгебра, содержащая все полуинтервалы вида $\left.(a;b\right]\in\mathbb{R}$, где $a$ и $b$, такие что $a,b\in(-\infty;+\infty)$ называется \textbf{борелевской \s-алгеброй} на числовой прямой и обозначается $\mathcal{B}(\mathbb{R})$

Элементы борелевской \s-алгебры называются \textbf{борелевскими множествами}

\ex Докажите, что следующие помдмножества числовой прямой являются борелевскими
\begin{enumerate}
    \item[\textbf{a)}] $\{b\}=\displaystyle\bigcap^\infty_{n=1}\underbrace{\left(b-\frac{1}{b}, b\right]}_{\in\mathcal{B}(\mathbb{R}) \text{ по опр.}}\in\mathcal{B}(\mathbb{R})$
    \item[\textbf{b)}] $(a, b)=\underbrace{(a, b]}_{\underset{\text{по опр.}}{\in\mathcal{B}(\mathbb{R})}}\setminus\underbrace{\{b\}}_{\underset{\text{по 1.}}{\in\mathcal{B}(\mathbb{R})}}\in\mathcal{B}(\mathbb{R})$
    \item[\textbf{c)}]  $[a, b)=\underbrace{\{a\}}_{\underset{\text{по 1.}}{\in\mathcal{B}(\mathbb{R})}}\cup\underbrace{(a, b)}_{\underset{\text{по 2.}}{\in\mathcal{B}(\mathbb{R})}}\in\mathcal{B}(\mathbb{R})$
    \item[\textbf{d)}]  $[a, b]=\underbrace{\{a\}}_{\underset{\text{по 1.}}{\in\mathcal{B}(\mathbb{R})}}\cup\underbrace{(a, b]}_{\underset{\text{по опр.}}{\in\mathcal{B}(\mathbb{R})}}\in\mathcal{B}(\mathbb{R})$
    
    % $\fk$ — $\sigma$-алгебра, 

    % $$F_1, A_2\in\underset{=\mathcal{B}(\mathcal{R})}{\fk}\implies
    % \begin{aligned}
    %     A_1\cup A_2\in\fk\\
    %     A_1\setminus A_2\in\fk\\
    %     A_1\cup A_2\in\fk
    % \end{aligned}$$
    \item[\textbf{e)}] $\mathbb{Q}=\displaystyle\bigcup^\infty_{k=1}\underbrace{\{r_k\}}_{\in\mathcal{B}(\mathbb{R})}\in\mathcal{B}(\mathbb{R})$
    \item[\textbf{f)}] $\underbrace{\mathbb{R}}_{=\Omega}\setminus \underbrace{\mathbb{Q}}_{\in\mathcal{B}(\mathbb{R})} \in \mathbb{Q}^c\in \mathcal{B}(\mathbb{R})$
\end{enumerate}

\definition Пусть задано непустое множество $\Omega$ и некоторая $\sigma$-алгебра $\fk$ подмножества в множестве $\Omega$. Тогда упорядоченная пара $(\Omega, \fk)$ называется \textbf{измеримым пространством}

\definition Пусть задано измеримое пространство $(\Omega,\fk)$. Функция $\xi:\Omega\rightarrow\mathbb{R},\xi=\xi(\omega),\omega\in\Omega$ называется измеримой относительно \s-алгебры $\fk$ функцией, если
$$\forall c\in\mathbb{R}\text{ множество }\{\omega\in\Omega:\xi(\omega)>c\}\in\fk$$
Функцию, измеримую относительно \s-алгебры $\fk$, также называют $\fk$-измеримой функцией

\definition В теории вероятностей $\fk$-измеримые функции также называются слчайными величинами, или $\fk$-измеримыми случайными величинами, если хотят подчернкуть тот факт, что они измеримы относительно \s-алгебры $\fk$

\ex $\Omega=\{a,b,c,d\}$, $\fk=\{\varnothing, \Omega, \{a,b\}, \{c,d\}\}$, $\xi:\Omega\rightarrow\mathbb{R},\eta:\Omega\rightarrow\mathbb{R}$
\begin{enumerate}
    \item $\xi$ — $\fk$-измерима
    
    $c=10^6:\ \{\omega\in\Omega:\xi(\omega)=c\}=\varnothing\in\fk$
    
    $c>1:\ \{\omega\in\Omega:\xi(\omega)>c\}=\varnothing\in\fk$

    $c=1:\ \{\omega\in\Omega:\xi(\omega)>1\}=\varnothing\in\fk$

    $-1<c<1:\ \{\omega\in\Omega:\xi(\omega)>c\}=\{c,d\}\in\fk$
    \item $\eta$ — не $\fk$-измерима
    
    $c=3.5:\ \{\omega\in\Omega:\eta(\omega)>3.5\}=\{d\}\notin\fk\Longrightarrow$ не $\fk$-измерима 
\end{enumerate}

\theorem Пусть $\xi:\Omega\rightarrow\mathbb{R}$ — $\fk$-измеримая функция. Тогда, $\forall a,b,c\in\mathbb{R}$
\begin{enumerate}
    \item $\{\omega\in\Omega:\xi(\omega)\leqslant c\}\in\fk$
    \item 
    \proof $\{\omega\in\Omega:\xi(\omega)\leqslant c\}=\underbrace{\Omega}_{\in\fk}\setminus\underbrace{\{\omega\in\Omega:\xi(\omega)>c\}}_{\in\fk}\in\fk$
\end{enumerate}

\definition Пусть задано измеримое пространство $(\Omega, \fk)$. Функция $\mathbb{P}\colon\fk\to[0,1]$ называется \textbf{вероятностной мерой}, если выполнены следующие условия:
\begin{enumerate}
    \item \textbf{(условие нормировки)} $P(\Omega)=1$
    \item \textbf{(условие счётной аддитивности, $\sigma$-аддитивность)} Для любой последовательности попарно непересекающихся множеств $A_1,\dots,A_n,\dots\subset\mathcal{F}$ верно
    $$\mathbb{P}\left(\bigcup^\infty_{i=1}A_i\right)=\sum^\infty_{i=1}\mathbb{P}(A_i)$$
    
\end{enumerate}

\definition Пусть $\xi:\Omega\rightarrow\mathbb{R}$ — $\fk$-измеримая функция. Функция
$$F_\xi(x)=\mathbb{P}(\{\omega\in\Omega:\xi(\omega)\leqslant x\}),x\in\mathbb{R}$$
называется \textbf{функцией распределения} случайной величины $\xi$

\end{document}
