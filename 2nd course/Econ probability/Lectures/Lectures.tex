\documentclass[a4paper, 10pt]{article}
\usepackage{header}

\title{\LARGE{Теория вероятности и математическая статистика—1}}
\author{Винер Даниил  \href{https://t.me/danya_vin}{@danya\_vin}}
\date{Версия от \today}
\begin{document}
\maketitle
\tableofcontents
\setlength{\parindent}{15pt}
\setlength{\parskip}{2mm}
\newpage
\section{Дискретное вероятностное пространство. Базовые теоремы вероятности}
\definition $\Omega=\{\omega_1,\ldots,\omega_k,\ldots\}$ называется \textit{пространством элементарных исходов}, где $w_i$ — элементарный исход\\[2mm]
\definition $A$ — любое подмножество $\Omega$\\[2mm]
\definition Событие называется \textit{достоверным}, если $A=\Omega$\\[2mm]
\comment К $A$ применимы те же опреации, что используются с множествами\\[2mm]
\definition Полная группа несовместных событий — такой набор событий, для которого выполняются такие условия:
\begin{equation*}
    A_i\cap A_j=\varnothing\ \forall i\ne j
\end{equation*}
\begin{equation*}
    \displaystyle\bigcup\limits_i A_i=\Omega
\end{equation*}

\axiom $\forall \omega_i\ \exists p_i\geqslant0,$ при этом $\displaystyle\sum_i p_i=1$\\[2mm]
\corollary $0\leqslant p_i\leqslant1$\\[2mm]
\definition $P(A)=\sum_{w_i\in A} P(w_i)$, где $P(w_i)=p_i$\\[2mm]
\indent $(\Omega, P)$ — вероятностное пространство в дискретном случае\\[2mm]
\textbf{Подходы к определению вероятностей}
\begin{enumerate}
    \item Априорный (предварительное знание)
    \item Частотный (предел ряда частот)
    \item Модельный (математическая модель)
\end{enumerate}
\subsection{Классическое определение вероятности}
\indent Имеет место, когда исходы равновероятны\\[2mm]
\definition
$$P(A)=\displaystyle\frac{|A|}{|\Omega|}$$

\definition $P(A)=\sum_{\omega_i\in A}p(\omega_i)$
\subsection{Теорема сложения}
\indent $P(A\cup B)=P(A)+P(B)-P(A\cap B)$\\[2mm]
\indent $P(A_1\cup\ldots\cup A_n)=\sum_{i=1}^{n} P(A_i)-\sum_{i<j} (A_i\cap A_j)+\ldots+(-1)^{n-1}P(A_1\cap\ldots\cap A_n)$
\subsection{Условная вероятность}
$P(A|B)=\displaystyle\frac{P(A\cap B)}{P(B)}\ \forall B: P(B)>0$
\subsection{Теорема умножения}
$P(A_1\cap\ldots\cap A_n)=P(A_1)\cdot P(A_2|A_1)\cdot P(A_3|A_1\cap A_2)\cdot\ldots\cdot P(A_n|A_1\cdot\ldots\cdot A_{n-1})$

\section{Независимые события. Апостериорная и условная вероятности}
Напомним классическое определение вероятности
$$P(A)=\sum_{\omega_i\in A}p(\omega_i)$$

\comment $P(\overline{A})=1-P(A),\ P(\Omega)=1$

\subsection{Задача об авариях}
Известно, что в $40\%$ аварий виноваты пьяные водители. Утверждается, что из этого следует, что в $60\%$ случаев виноваты трезвые водители

Пусть есть такие события: $A$ — водитель пьян, $B$ — водитель трезв, $C$ — авария случилась

Формально, задача выглядит так: $P(A|C)=0.4\Longrightarrow P(B|C)=0.6$

Пусть $P(B)=0.05$

$\displaystyle\frac{P(C|A)}{P(C|B)}=\frac{P(C\cap A)/P(A)}{P(C\cap B)}=\frac{P(A|C)P(C)}{P(B|C)P(C)}\cdot\frac{P(B)}{P(A)}=\frac{0.4}{0.6}\cdot\frac{0.95}{0.05}\approx 12.7$

\subsection{Независимость событий}
\definition (интуитивное) События  $A\text{ и }B$ \textbf{независимы}, если $P(A|B)=P(A)$

\definition События $A\text{ и }B$ называются попарно независимыми, если:
\begin{equation*}
    \begin{aligned}
        P(A\cap B)&=P(A)\cdot P(B)\\
        P(A|B)P(B)&=P(A)\cdot P(B)\text{ — вытекает интуитивное определение}
    \end{aligned}
\end{equation*}

\definition События $A_1,\ldots, A_n$ независимы в совокупности, если:
\begin{equation*}
    \begin{aligned}
    \forall i_1<\ldots<i_k<\ldots<i_n\ \forall k=1,\ldots,n:\\ P(A_{i_1}\cap A_{i_2}\cap\ldots\cap A_{i_k})=P(A_{i_1})\cdot P(A_{i_2})\cdot \ldots\cdot P(A_{i_k})
    \end{aligned}
\end{equation*}

\comment  Для $A_1,A_2,A_3$:
\begin{equation*}
    \begin{aligned}
        P(A_1\cap A_2)&=P(A_1)\cdot P(A_2)\\
        P(A_2\cap A_3)&=P(A_2)\cdot P(A_3)\\
        P(A_1\cap A_3)&=P(A_1)\cdot P(A_3)\\
        P(A_1\cap A_2\cap A_3)&=P(A_1)\cdot P(A_2)\cdot P(A_3)
    \end{aligned}
\end{equation*}

\subsection{Пример зависимых событий в совокупности}
Положим, что у нас есть тетраэдр, каждая сторона которого покрашена в некоторые комбинации цветов: $A$ — красный, $B$ — желтый, $C$ — зеленый, а четвертая покрашена во все три цвета, тогда вероятности:
\begin{equation*}
    \frac{1}{4}=P(A\cap B\cap C)\ne P(A)\cdot P(B)\cdot P(C)=\frac{1}{8}
\end{equation*}
Таким образом, события зависимы в совокупности

\comment Если $A_1,\ldots, A_n$ независимы в совокупности, то над любым из событий можно поставить знак отрицания, тогда система останется независимой

\ex Есть события $A_1,A_2,A_3$ — независимы в совокупности, тогда $\overline{A_1},A_2,A_3$ — тоже независимы в совокупности

Проверим данный пример. 

\begin{equation*}
    \begin{aligned}
        P(\overline{A_1}\cap A_2\cap A_3)&=P((A_2\cap A_1)\setminus A_1)\\
        &=P(A_2)P(A_3)-P(A_1\cap A_2\cap A_3)\\
        &=P(A_2)P(A_3)-P(A_1)P(A_2)P(A_3)\\
        &=P(\overline{A_1})P(A_2)P(A_3)
    \end{aligned}
\end{equation*}
Теперь разберемся с двумя событиями
\begin{equation*}
    \begin{aligned}
        P(\overline{A_1}\cap A_2)&=P(\overline{A_1})P(A_2)\\
        P(\overline{A_2}\cap A_1)&=P(A_2\setminus A_1)\\
        &=P(A_2)-P(A_2\cap A_1)=P(A_2)-P(A_2)P(A_1)\\
        &=P(A_2)(1-P(A_1))
    \end{aligned}
\end{equation*}

\subsection{Простейший варинат ЗБЧ. Неизбежность технологических катастроф}
Имеется $n$ узлов, а $A_1,\ldots,A_n$ — события, где $A_i$ означает, что $i$-ый узел вышел из строя

Очевидно, что $P(\text{хотя бы один узел выйдет из строя})=P(A_1\cup \ldots \cup A_n)$, тогда $\forall i: 0<\varepsilon\leqslant P(A_i)<1$ выполняется:

% Вероятность стремится к 1 при увеличении количества узлов
$$1\geqslant P(A_1\cup \ldots \cup A_n)=1-P(\overline{A_1}\cap \ldots\cap\overline{A_n})=1-\displaystyle\prod_{i=1}^{n} \underbrace{P(\overline{A_i})}_{\leqslant 1-\varepsilon}\geqslant \lim\limits_{n\mapsto\infty}1-(1-\varepsilon)^n=1$$
% где каждое $P(\overline{A_i})=1-P(A_i)\leqslant 1-\varepsilon$

Это означает, что вероятность технологических катастроф при количестве узлов, стремящемся в бесконечность, равна $100\%$
\subsection{Формула полной вероятности}
Пусть $\{H_i\}$ — полная группа несовместных событий (разбиение $\Omega$)

Некоторые свойства:
\begin{itemize}
    \item $H_i\cap H_j=\varnothing\ \forall i\ne j$ — несовместность\\[1mm]
    \item $\displaystyle\bigcup_{i=1}^{n}H_i=\Omega$ — полнота
\end{itemize}

\theorem Тогда, $P(A)=\sum_{i=1}^{n}P(A|H_i)P(H_i)$

\proof
\begin{equation*}
    \begin{aligned}
        P(A)&=P\left(\bigcup_{i=1}^{n}(A\cap H_i)\right)\\
        &=\sum_{i=1}^{n}P(A\cap H_i)\\
        &=\sum_{i=1}^{n} P(A|H_i)\cdot P(H_i)
    \end{aligned}
\end{equation*}\qed

\subsection{Формула Байеса. Апосториорные вероятности}
\begin{equation*}
    \begin{aligned}
        P(H_k|A)&=\frac{P(A|H_k)\cdot P(H_k)}{P(A)}\\
        &=\frac{P(A|H_k)\cdot P(H_k)}{\sum_{i=1}^{n} P(A|H_i)P(H_i)}
    \end{aligned}
\end{equation*}

\subsection{Задача про неизличимые заболевания}
Требуется вычислить, какова вероятность, что человек, который получил положительный тест на СПИД, на самом деле не болен

$P(\text{СПИД})=0.03$ — вероятность быть носителем СПИДа

$P(+|\text{ СПИД})=0.98$ — чувствительность теста, т.е. тест будет положительным, если у человека есть СПИД, с вероятностью $0.98$

$P(+|\overline{\text{СПИД}})=0.01=1-\text{специфичность}$, т.е. тест будет положительным, если у человека нет СПИДа, с вероятностью $0.01$

\begin{equation*}
    \begin{aligned}
        P(\text{СПИД}|+)&=\displaystyle\frac{P(+|\text{СПИД})P(\text{СПИД})}{P(+|\text{СПИД})P(\text{СПИД})+P(+|\overline{\text{СПИД}})P(\overline{\text{СПИД}})}\\
        &=\frac{0.98\cdot0.03}{0.98\cdot0.03+0.01\cdot0.97}\\
        &\approx0.75
    \end{aligned}
\end{equation*}
То есть, \textbf{каждый четвертый} человек с положительным СПИД-тестом \textbf{здоров}

\section{Схема Бернулли. Теорема Пуассона. Случайные величины}
Рассмотрим задачу проведения социологических опросов. Допустим, проводится опрос <<Употребляет ли человек наркотики?>>. Предполагается, что не все респонденты, даже на условиях анонимности, дадут честный ответ. Для решения этой проблемы иногда задаются косвенные вопросы, мы же рассмотрим следующий подход.

Пусть человек перед ответом берет из урны случайный шарик. В урне лежит $N$ шариков трех цветов: белый, черный и красный. Если человек достал белый, то его ответ на вопрос просто число <<0>>, если черный — <<1>>, если красный — респондент должен сказать правду: употребляет ли он наркотики. Тогда, все ответы разделятся на три категории, которые можно описать, как
$$N=N_\text{к}+N_{\text{ч}}+N_{\text{б}}$$

Пусть $p$ — вероятность употребления наркотиков респондентом, тогда по формуле полной вероятности получаем
$$P(1)=P(1|\text{б})P(\text{б})+P(1|\text{ч})P(\text{ч})+P(1|\text{к})P(\text{к})$$

Отсюда следует, что $p=\displaystyle\frac{P(1)N-N_{\text{ч}}}{N_{\text{к}}}$

\ex Если оказалось, что $P(1)=0.6$, это означает, что $p=\frac{1}{2}$

\subsection{Схема Бернулли. Биномиальное распредление вероятности}
Допустим, что мы проводим $n$ независимых опытов, каждый из которых может закончится успехом (1) или неудачей (0). Пусть вероятность успеха $P(1)=p$, тогда $P(0)=1-p=q$

Упорядочим результаты экспериментов в последовательность длины $n$, тогда всего исходов $|\Omega|=2^n$, то есть последоватльности длины $n$, состоящие из 0 и 1. Заметим, что они \textbf{НЕ равновероятны}

Пользуясь тем, что события независимы, получаем, что $$P(\underbrace{1,\ldots,1}_{k},\underbrace{0,\ldots,0}_{n-k})=p^k(1-p)^{n-k}$$

Заметим, что исходов, в которых $k$ успехов, существует $C_n^k$, при этом
$$P(\text{$k$ успехов в $n$ испытаниях})=C_n^k p^k(1-p)^{n-k}$$

\definition Набор чисел $\{C_n^k p^k(1-p)^{n-k}|k=0,1,\ldots,n\}$ называется \textbf{биномиальным распределением вероятности}

\subsection{Исследование Пуассона}
Французский математик Симеон Дэни Пуассон проводил исследование, в котором пытался понять насколько справедливы обвинительные приговоры, которые выносят судьи присяжных во Франции.

Присяжных $n=12$ человек. Для того, чтобы человек был признан виновынм, нужно 7 голосов <<ЗА>>. Пуассон знал лишь процент обвинительных приговоров и количество обвинительных приговоров, вынесенных ровно 7 присяжными заседателями. Пусть $p$ — вероятность верного решения присяжного (неизвестный нам параметр). Верное решение — когда улик достаточно для обвинения - присяжный голосует за \textit{виновность}, не достаточно — за \textit{невиновность}

Пусть есть событие $A$ — улик достаточно для обвинения. Заметим, что $P(A)$ нам неизвестно.

Очевидно, что частота обвинительных приговоров$=P(\text{обвинение})$, тогда

$$P(\text{обвинение})=P(\text{обвинение}|A)P(A)+P(\text{обвинение}|\overline{A})P(\overline{A})$$

Теперь введем вероятность, что человек был обвинен, когда за это проголосовало ровно 7 присяжных:
\begin{equation*}
    \begin{aligned}
        P(\text{обв. 7})&=P(\text{обв. 7}|A)P(A)+P(\text{обв. 7}|\overline{A})P(\overline{A})\\
        &=C_{12}^7 p^7(1-p)^5P(A)+C_{12}^5(1-p)^7p^5(1-P(A))
    \end{aligned}
\end{equation*}

Тогда, $P(\text{обвинение})=\left(\sum_{k=7}^{12} C_{12}^k p^k(1-p)^{12-k}\right)P(A)+\left(\sum_{k=7}^{12} C_7^{12-k}(1-p)^kp^{12-k}\right)(1-P(A))$

Пуассон решил, что $p\approx\displaystyle\frac{2}{3}$. Однако, это было ошибочным решением. Математик считал, что присяжные выносят решения независимо друг от друга, что в корни неверно. А так как Пуассон опирался на схему Бернулли, которая работает только для \textit{независимых событий}, то и результат его исследования получился ложным

При этом, Пуассона не смутило, что в Англии для вынесения обвинительного приговора нужно, чтобы все 12 присяжных проголосавали за обвинение. Если бы решение Пуассона было верным, тогда вероятность обвинительных приговоров должна быть $\approx\left(\frac{2}{3}\right)^{12}$, что не соответствовало действительности, ведь в Англии было больше обвинительных приговоров чем во Франции

\subsection{Теорема Пуассона}
\theorem Пусть имеются серии испытаний. В каждой серии $n$ испытаний и $p_n$ — вероятность успеха. Также, пусть $n\rightarrow\infty$ и $p_n\rightarrow0$, причем $np_n\rightarrow\lambda$

Тогда, 
\begin{equation*}
    \forall k\ P(k\text{ успехов в $n$ испытаниях})=C_n^kp^k(1-p)^{n-k}\rightarrow\frac{\lambda^k}{k!}e^{-\lambda}
\end{equation*}

\proof Пусть $\mu_n$ — число успехов в $n$ испытаниях Бернулли с вероятностью успеха $p_n$, а также $\lambda_n=n\cdot p_n$. По условию, $\lambda^k_n\rightarrow\lambda>0$. Подставим $p_n=\displaystyle\frac{\lambda_n}{n}$ в формулу Бернулли. Тогда,

\begin{equation*}
    \begin{aligned}
        P(\mu_n=m)&=\displaystyle\frac{n!}{m!(n-m)!}p_n^m (1-p_n)^{n-m}\\
        P(\mu_n=m)&=\displaystyle\frac{n\cdot\ldots\cdot(n-m+1)}{n^k}\left(\frac{\lambda_n}{m}\right)^m\frac{(1-\frac{\lambda_n}{n})^n}{(1-\frac{\lambda_n}{n})^m}\\
        &=\displaystyle\frac{n\cdot\ldots\cdot(n-m+1)}{n^k}\left(\frac{\lambda_n}{m}\right)^m\left(1-\frac{\lambda_n}{n}\right)^n\cdot\frac{1}{(1-\frac{\lambda_n}{n})^m}\\
        &=\underbrace{\displaystyle\frac{n\cdot\ldots\cdot(n-m+1)}{n^k}}_{\lim\limits_{n\rightarrow\infty}=1}\frac{\lambda_n^m}{m!}\underbrace{\left(1-\frac{\lambda_n}{n}\right)^n}_{e^{-\lambda}}\cdot\underbrace{\left(1-\frac{\lambda_n}{n}\right)^{-m}}_{\lim\limits_{n\rightarrow\infty}=1}\\
        &=\frac{\lambda^k}{k!}e^{-\lambda}
    \end{aligned}
\end{equation*}

\definition Набор чисел $\left\{\frac{\lambda^k}{k!}e^{-\lambda}\colon k=0,1,\ldots,n\right\}$ называется \textbf{распределением Пуассона}

\comment Величина $\frac{\lambda^k}{k!}e^{-\lambda}$ устойчива при зависимых событиях, а также при ошибках предпосылок (таких, какие допустил Пуассон)

\comment $\forall B$ — множество значений, которые может принимать число успехов
$$\left|P(\mu\in B)-\sum_{m\in B}e^{-\lambda}\frac{\lambda^m}{m!}\right|\leqslant\min{(p,np^2)}$$

\subsection{Случайные величины. Математическое ожидание}
\definition Случайная величина $\xi:\Omega\rightarrow\mathbb{R}$

$\xi$ имеет биномиальное распределение, если
$$\xi\in\{0,1,\ldots,n\}\ P(\xi=k)=C_n^k p^k(1-p)^{n-k},$$
где $\{0,1,\ldots,n\}$ — число успехов в $n$ испытаниях

$P(\xi\in B)=\sum_{\omega} P(\omega:\xi(\omega)\in B)$

\definition Математическим ожиданием величины $\xi$ называется величина $$\mathbb{E}(\xi)=\sum_{\omega} P(\xi)\cdot\xi(\omega),$$
если этот ряд сходится абсолютно, то есть $\sum\left|\xi(\omega)\right|P(\omega)$

\theorem $\mathbb{E}(\xi)=\sum_{k} P(\xi=a_k)$ 

% (доказать)

% Доказать, что $\mathbb{E}(\xi+\nu)=\mathbb{E}\xi+\mathbb{E}\nu$


% Эта штука устойчива при зависимых событиях, а также при ошибках предпосылок

% Эта вероятность называется \textbf{распределением Пуассона}

























\end{document}
